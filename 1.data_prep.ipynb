{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8b51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from funcz import clean_column_names\n",
    "import duckdb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "engine = duckdb.connect('my_db.duckdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de968428",
   "metadata": {},
   "source": [
    "# 1.1: Data Prep: Demographic Data with the expontential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b2708e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cb = pd.read_excel(os.path.join(os.getcwd(), 'demographic_data', 'demographic.xlsx'))\n",
    "cb = cb[cb[\"CD Type\"] == \"CD\"]\n",
    "\n",
    "df_dict = {\"year\":[], \"community_board\":[], \"metric\":[],\"value\":[]}\n",
    "metric = pd.read_csv(r'C:\\Users\\joe tech\\Desktop\\NYC\\demographic_data\\column_lookup.csv')\n",
    "metric_list = list(metric[\"abbrev\"].unique())\n",
    "cb = cb[cb[\"CD Type\"] == \"CD\"]\n",
    "\n",
    "cb = cb.drop(columns=[\"Orig Order\", \"GeoType\", \"Borough\",\"GeoID\",\"BCT2020\",\"CD Type\",\"NTA Type\"])\n",
    "\n",
    "columns = cb.columns.to_list()\n",
    "for n in columns:\n",
    "    if n[-1] == \"P\":\n",
    "        cb = cb.drop(columns=[n])\n",
    "cb.columns\n",
    "columns = cb.columns.to_list()\n",
    "cb.reset_index(inplace=True)\n",
    "for i ,row in cb.iterrows(): \n",
    "    for n in columns: \n",
    "            try:\n",
    "                \n",
    "                if n[:-3] in metric_list and n[-3:] in (\"_10\", \"_20\"):\n",
    "\n",
    "                    df_dict[\"metric\"].append(n[:-3])\n",
    "                    df_dict[\"value\"].append(cb[n].iloc[i])\n",
    "                    df_dict[\"community_board\"].append(cb[\"Name\"].iloc[i])\n",
    "                    if n[-3:] == \"_10\":\n",
    "                        df_dict[\"year\"].append(2010)\n",
    "                    elif n[-3:] == \"_20\":\n",
    "                        df_dict[\"year\"].append(2020)\n",
    "            except Exception as e:\n",
    "                print(f\"error {e}\")\n",
    "\n",
    "demo_df = pd.DataFrame(df_dict)\n",
    "\n",
    "demo_df[\"metric\"] = demo_df[\"metric\"].map({\n",
    "    \"Pop\": \"Population\",\n",
    "    \"HHPop\": \"Household Population\",\n",
    "    \"AvHHSz\": \"Average Household Size\",\n",
    "    \"PopU18\": \"Under 18 Population\",\n",
    "    \"Hsp\": \"Hispanic Population\",\n",
    "    \"WNH\": \"White Population\",\n",
    "    \"BNH\": \"Black Population\",\n",
    "    \"ANH\": \"Asian Population\",\n",
    "    \"ONH\": \"Other Population\",\n",
    "    \"HUnits\": \"Housing Units\",\n",
    "    \"OcHU\": \"Occupied Housing\"\n",
    "})\n",
    "\n",
    "demo_df\n",
    "predicted_rows = []\n",
    "\n",
    "\n",
    "grouped = demo_df.groupby(['community_board', 'metric'])\n",
    "\n",
    "for (cb, m), group in grouped:\n",
    "    if not set([2010, 2020]).issubset(group['year'].values):\n",
    "        continue\n",
    "\n",
    "    value_2010 = group.loc[group['year'] == 2010, 'value'].values[0]\n",
    "    value_2020 = group.loc[group['year'] == 2020, 'value'].values[0]\n",
    "\n",
    "    if value_2010 == 0 or value_2020 == 0:\n",
    "        continue\n",
    "\n",
    "    r = (1 / 10) * np.log(value_2020 / value_2010)\n",
    "\n",
    "    for year in range(2006, 2025):\n",
    "        if year in (2010, 2020):\n",
    "            continue\n",
    "\n",
    "        t = year - 2010\n",
    "        predicted_value = value_2010 * np.exp(r * t)\n",
    "\n",
    "        predicted_rows.append({\n",
    "            'year': year,\n",
    "            'community_board': cb,\n",
    "            'metric': m,\n",
    "            'value': predicted_value\n",
    "        })\n",
    "\n",
    "\n",
    "predicted_df = pd.DataFrame(predicted_rows)\n",
    "\n",
    "\n",
    "demo_df = pd.concat([demo_df, predicted_df], ignore_index=True)\n",
    "demo_df.sort_values(by=['community_board', 'metric', 'year'], inplace=True)\n",
    " \n",
    "demo_df = clean_column_names(demo_df)\n",
    "demo_df[\"community_board\"] = demo_df[\"community_board\"].str.replace(\"District\",repl=\"Board\")\n",
    "demo_df.to_sql(con=engine,name=\"demo_data\",if_exists=\"replace\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d130b3e",
   "metadata": {},
   "source": [
    "# 1.2.1 : Data Prep School Data: Pre 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16803f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-08 SCHOOL SUPPORT ORGANIZATION\n",
      "2007-08 QUALITY REVIEW SCORE\n",
      "2008-09 SCHOOL SUPPORT ORGANIZATION\n",
      "2008-09 STATE ACCOUNTABILITY STATUS\n",
      "2009-2010 OVERALL GRADE\n",
      "2009-2010 OVERALL SCORE\n",
      "2009-2010 ENVIRONMENT CATEGORY SCORE\n",
      "2009-2010 ENVIRONMENT GRADE\n",
      "2009-2010 PERFORMANCE CATEGORY SCORE\n",
      "2009-2010 PERFORMANCE GRADE\n",
      "2009-2010 PROGRESS CATEGORY SCORE\n",
      "2009-2010 PROGRESS GRADE\n",
      "2009-2010 ADDITIONAL CREDIT\n",
      "2010-2011 OVERALL GRADE\n",
      "2010-2011 OVERALL SCORE\n",
      "2010-2011 ENVIRONMENT CATEGORY SCORE\n",
      "2010-2011 ENVIRONMENT GRADE\n",
      "2010-2011 PERFORMANCE CATEGORY SCORE\n",
      "2010-2011 PERFORMANCE GRADE\n",
      "2010-2011 PROGRESS CATEGORY SCORE\n",
      "2010-2011 PROGRESS GRADE\n",
      "2010-2011 ADDITIONAL CREDIT\n",
      "2011-2012 OVERALL GRADE\n",
      "2011-2012 OVERALL SCORE\n",
      "2011-12 OVERALL PERCENTILE\n",
      "2011-2012 PROGRESS CATEGORY SCORE\n",
      "2011-2012 PROGRESS GRADE\n",
      "2011-2012 PERFORMANCE CATEGORY SCORE\n",
      "2011-2012 PERFORMANCE GRADE\n",
      "2011-2012 ENVIRONMENT CATEGORY SCORE\n",
      "2011-2012 ENVIRONMENT GRADE\n",
      "2011-2012 COLLEGE AND CAREER READINESS SCORE\n",
      "2011-2012 COLLEGE AND CAREER READINESS GRADE\n",
      "2011-2012 ADDITIONAL CREDIT\n",
      "2012-2013 OVERALL GRADE\n",
      "2012-2013 OVERALL SCORE\n",
      "2012-13 OVERALL PERCENTILE\n",
      "2012-2013 PROGRESS CATEGORY SCORE\n",
      "2012-2013 PROGRESS GRADE\n",
      "2012-2013 PERFORMANCE CATEGORY SCORE\n",
      "2012-2013PERFORMANCE GRADE\n",
      "2012-2013 ENVIRONMENT CATEGORY SCORE\n",
      "2012-2013 ENVIRONMENT GRADE\n",
      "2012-2013 COLLEGE AND CAREER READINESS SCORE\n",
      "2012-2013 COLLEGE AND CAREER READINESS GRADE\n",
      "2012-2013 ADDITIONAL CREDIT\n"
     ]
    }
   ],
   "source": [
    "school_data_folder = Path.cwd() / 'school_data'\n",
    "school_performance_df = pd.DataFrame()\n",
    "\n",
    "years = [\"2006-2007\",\"2007-2008\",\"2008-2009\",\"2009-2010\",\"2010-2011\",\"2011-2012\",\"2012-2013\"]\n",
    "counter = 0\n",
    "\n",
    "for file in school_data_folder.iterdir():\n",
    "    if \"pr\" in file.name:\n",
    "        name = file.name.split(\"_\")\n",
    "        year = f\"20{name[1]}-20{name[2].replace('.csv', '')}\"\n",
    "        year_alt = f\"20{name[1]}-{name[2].replace('.csv', '')}\"\n",
    "        #print(f\"start {year_alt}\")\n",
    "        \n",
    "        df = pd.read_csv(file)\n",
    "        df = df[df[\"SCHOOL LEVEL*\"].isin([\"Elementary\", \"Middle School\", \"High School\"])]\n",
    "       \n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        map_dictionary = {}\n",
    "        columns = df.columns.to_list()\n",
    "        \n",
    "        for n in columns:\n",
    "            year = f\"20{name[1]}-20{name[2].replace('.csv', '')}\"\n",
    "            year_alt = f\"20{name[1]}-{name[2].replace('.csv', '')}\"\n",
    "\n",
    "            if (year in n):\n",
    "                print(n)\n",
    "                map_dictionary[n] = n.replace(year, \" \").strip(\" \")\n",
    "            elif year_alt in n:\n",
    "                print(n)\n",
    "                map_dictionary[n] = n.replace(year_alt, \" \").strip(\" \")\n",
    "            elif re.search(r\"20\\d{2}-\\d{2,4}\", n):\n",
    "                df.drop(columns=[n], inplace=True)\n",
    "\n",
    "        df.rename(columns=map_dictionary, inplace=True)\n",
    "        df = df.reset_index()\n",
    "        df[\"year\"] = years[counter]\n",
    "        school_performance_df = pd.concat([school_performance_df, df], ignore_index=True)\n",
    "        counter += 1\n",
    "school_performance_df\n",
    "\n",
    "school_performance_df = school_performance_df[[\"DBN\", \"OVERALL SCORE\",\"year\"]]\n",
    "school_performance_df.rename(columns={\"OVERALL SCORE\":\"score\"},inplace=True)\n",
    "school_performance_df.rename(columns={'year':'years'},inplace=True)\n",
    "school_performance_df = clean_column_names(school_performance_df)\n",
    "school_performance_df[\"score\"].replace(to_replace=\".\", value=np.nan,inplace=True)\n",
    "school_performance_df[\"score\"] = school_performance_df[\"score\"].astype(float)\n",
    "school_performance_df[\"score\"] = school_performance_df[\"score\"]/100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1948e79",
   "metadata": {},
   "source": [
    "# 1.2.2: Data Prep School Data: Post 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48423374",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data={'DBN':[],'score':[],'year':[]})\n",
    "\n",
    "years = [\"2014-2015\", \"2015-2016\", \"2016-2017\", \"2017-2018\", \"2018-2019\", \"2022-2023\", \"2023-2024\"]\n",
    "folder = Path.cwd() / 'new_school_data'\n",
    "\n",
    "files = [f for f in folder.iterdir() if f.is_file()]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    year = years[i // 2]  \n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name=\"Student Achievement\", header=0)  \n",
    "        df = df[[\"DBN\", \"Student Achievement - Section Score\"]]\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            df = pd.read_excel(file, sheet_name=\"Student Achievement\", header=1)\n",
    "            df = df[[\"DBN\", \"Student Achievement - Section Score\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"error in file {file}: {e}\")\n",
    "       \n",
    "    df.rename(columns={\"Student Achievement - Section Score\": \"score\"},inplace=True)\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    data = pd.concat([data,df], ignore_index=True)\n",
    "data.rename(columns={\"year\":\"years\"},inplace=True)\n",
    "data = clean_column_names(data)\n",
    "data[\"score\"].replace(to_replace=\".\", value=np.nan,inplace=True)\n",
    "data[\"score\"] = data[\"score\"].astype(float)\n",
    "data[\"score\"] = data[\"score\"]/4.99\n",
    "school_performance_df = pd.concat([school_performance_df, data],ignore_index=True)\n",
    "\n",
    "school_performance_df = clean_column_names(school_performance_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26b0ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dbns = school_performance_df[\"dbn\"].dropna().unique()\n",
    "missing_years = [\"2019-2020\", \"2020-2021\", \"2021-2022\"]\n",
    "missing_df = pd.DataFrame([(dbn, np.nan, year) for dbn in all_dbns for year in missing_years],\n",
    "                          columns=[\"dbn\", \"score\", \"years\"])\n",
    "school_performance_df = pd.concat([school_performance_df, missing_df], ignore_index=True)\n",
    "school_performance_df[\"year_order\"] = school_performance_df[\"years\"].str.split(\"-\").str[0].astype(int)\n",
    "school_performance_df.sort_values(by=[\"dbn\", \"year_order\"], inplace=True)\n",
    "school_performance_df[\"interpolated\"] = school_performance_df.groupby(\"dbn\")[\"score\"].transform(lambda x: x.interpolate())\n",
    "mask = school_performance_df[\"year\"].isin(missing_years) & school_performance_df[\"score\"].isna()\n",
    "school_performance_df.loc[mask, \"score\"] = school_performance_df.loc[mask, \"interpolated\"]\n",
    "school_performance_df.drop(columns=[\"interpolated\", \"year_order\"], inplace=True)\n",
    "school_performance_df.to_sql(name=\"school_performance\",con=engine,if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e0611",
   "metadata": {},
   "source": [
    "# 1.3.1 Data Prep: Keeping Track Online Data\n",
    "As they are all aggregated the same way from the same source, minimal data prep is needed. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e84ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_board_mapping = pd.read_csv(Path.cwd() / 'keep_track_online' / f'community_boards.csv')\n",
    "names = [\"median_income\",\"rent\",\"unemployment_rate\"]\n",
    "for n in names:\n",
    "    df = pd.read_csv(Path.cwd() / 'keep_track_online' / f'{n}.csv')\n",
    "    df[\"Data\"] = pd.to_numeric(df[\"Data\"], errors=\"coerce\")\n",
    "    df.rename(columns={\"Data\":n,\"TimeFrame\": \"year\"},inplace=True)\n",
    "    df = df.merge(right=community_board_mapping,left_on=[\"Location\"], right_on=[\"Community Name\"],how=\"left\")\n",
    "    df.drop(columns=[\"DataFormat\",\"Fips\",\"Location\",\"Community Name\"],inplace=True)\n",
    "    df.rename(columns={\"Data\": n,\"TimeFrame\": \"year\"},inplace=True)\n",
    "    df = clean_column_names(df)\n",
    "    df.to_sql(con=engine,name=f\"{n}_data\",if_exists=\"replace\",index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16379c39",
   "metadata": {},
   "source": [
    "## 1.3.2 Filling it Missing Values for Keeping Track Online Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dddfe35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =   '''\n",
    "  -------------------------------------------------------------------------------------------------------------------\n",
    "  --Base CTE\n",
    "  -------------------------------------------------------------------------------------------------------------------\n",
    "  WITH base_CTE AS (\n",
    "  SELECT \n",
    "  cb.community_board,\n",
    "  ye.year\n",
    "  FROM community_board_data cb\n",
    "  CROSS JOIN generate_series(2006, 2024) AS ye(year)\n",
    "  )\n",
    "  SELECT DISTINCT b.*, urd.unemployment_rate, rd.rent, mid.median_income FROM base_CTE b\n",
    "  LEFT JOIN median_income_data mid ON b.community_board = mid.community_board AND b.YEAR = mid.YEAR\n",
    "  LEFT JOIN unemployment_rate_data urd  ON b.community_board = urd.community_board  AND b.YEAR = urd.YEAR\n",
    "  LEFT JOIN rent_data rd ON b.community_board = rd.community_board  AND b.YEAR = rd.YEAR\n",
    "  '''\n",
    "\n",
    "try:\n",
    "   df = pd.read_sql(con=engine,sql= query)\n",
    "\n",
    "except:\n",
    "    print(\"yuh\")\n",
    "    cb_df = pd.read_csv(Path.cwd() / 'geographic_district_data' / f'community_boards.csv')\n",
    "    cb_df = cb_df[[\"Community Board\", \"Borough\", \"Latitude\", \"Longitude\"]]\n",
    "    cb_df[\"Community Board\"] = cb_df[\"Borough\"] + \" \" + cb_df[\"Community Board\"]\n",
    "    cb_df = clean_column_names(cb_df)\n",
    "    cb_df.to_sql(name=\"community_board_data\", con=engine, if_exists=\"replace\",index=False)\n",
    "    metrics = [\"median_income\", \"rent\", \"unemployment_rate\"]\n",
    "    df = pd.read_sql(con=engine,sql=query)\n",
    "\n",
    "for metric in metrics:\n",
    "    df[metric] = (\n",
    "        df.groupby('community_board')[metric]\n",
    "          .transform(lambda g: g.interpolate(method='linear', limit_direction='both'))\n",
    "    )\n",
    "\n",
    "    df[metric] = (\n",
    "        df.groupby('community_board')[metric]\n",
    "          .transform(lambda g: g.ffill().bfill())\n",
    "    )\n",
    "\n",
    "    year_medians = df.groupby('year')[metric].transform('median')\n",
    "    df[metric] = df[metric].fillna(year_medians)\n",
    "\n",
    "    upload_df = df[['community_board', 'year', metric]].drop_duplicates().copy()\n",
    "    upload_df.to_sql(con=engine, name=f\"{metric}_data\", if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50fd7a",
   "metadata": {},
   "source": [
    "# 1.4 Crime Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f622e6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m crime_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpct\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(crime_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpct\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInt64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m crime_df \u001b[38;5;241m=\u001b[39m crime_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpct\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m \u001b[43mcrime_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrime_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[1;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    839\u001b[0m     )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[0;32m    843\u001b[0m         frame,\n\u001b[0;32m    844\u001b[0m         name,\n\u001b[0;32m    845\u001b[0m         if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[0;32m    846\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    847\u001b[0m         index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m    848\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    849\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    850\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    851\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    852\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[0;32m    854\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:2850\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   2839\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2841\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[0;32m   2842\u001b[0m     name,\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2848\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   2849\u001b[0m )\n\u001b[1;32m-> 2850\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39minsert(chunksize, method)\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:984\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfail\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    986\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:970\u001b[0m, in \u001b[0;36mSQLTable.exists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpd_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:2865\u001b[0m, in \u001b[0;36mSQLiteDatabase.has_table\u001b[1;34m(self, name, schema)\u001b[0m\n\u001b[0;32m   2854\u001b[0m wld \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2855\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   2856\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[0;32m   2857\u001b[0m \u001b[38;5;124m    name\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;124m    AND name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwld\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m-> 2865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joe tech\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\sql.py:2672\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery must be a string unless using sqlalchemy.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2671\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[1;32m-> 2672\u001b[0m cur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mConnectionException\u001b[0m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "crime_types = ['major','non-major','misdemeanor','violation']\n",
    "crime_df = None\n",
    "for n in crime_types:\n",
    "    df = pd.read_excel(Path.cwd() / 'crime_data' / f'{n}.xls')\n",
    "    \n",
    "    filled = df[\"PCT\"][~df[\"PCT\"].isna()]\n",
    "    filled.to_dict()\n",
    "\n",
    "    for key, val in filled.items():\n",
    "        df[\"PCT\"].iloc[key: key+8] = val\n",
    "\n",
    "    df_melted = pd.melt(df, id_vars=['PCT', 'CRIME'], var_name='Year', value_name='Count')\n",
    "    df_melted[\"crime_category\"] = n\n",
    "    if isinstance(crime_df, pd.DataFrame):\n",
    "        \n",
    "        crime_df = pd.concat([crime_df,df_melted],ignore_index=True)\n",
    "    else:\n",
    "        crime_df = df_melted\n",
    "crime_df.astype({'PCT': 'object'})\n",
    "crime_df = clean_column_names(crime_df)\n",
    "crime_df = crime_df[~crime_df[\"crime\"].str.contains(\"TOTAL\", case=False, na=False)]\n",
    "crime_df[\"pct_2\"] = crime_df[\"pct\"].apply(\n",
    "    lambda x: x.split('.')[0] if isinstance(x, str) and len(x.split('.')[0]) <= 4 else None\n",
    ")\n",
    "crime_df['pct'] = pd.to_numeric(crime_df['pct'], errors='coerce').astype('Int64')\n",
    "crime_df = crime_df.dropna(subset=['pct'])\n",
    "crime_df.to_sql(con=engine,name=\"crime_data\",if_exists=\"replace\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c529dbe",
   "metadata": {},
   "source": [
    "# Uploading Dimension Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e858ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CommunityBoard & Borough Table##\n",
    "\n",
    "cb_df = pd.read_csv(Path.cwd() / 'geographic_district_data' / f'community_boards.csv')\n",
    "cb_df = cb_df[[\"Community Board\", \"Borough\", \"Latitude\", \"Longitude\"]]\n",
    "cb_df[\"Community Board\"] = cb_df[\"Borough\"] + \" \" + cb_df[\"Community Board\"]\n",
    "cb_df = clean_column_names(cb_df)\n",
    "cb_df.to_sql(name=\"community_board_data\", con=engine, if_exists=\"replace\",index=False)\n",
    "\n",
    "borough_df = pd.DataFrame({\"borough\": list(cb_df[\"borough\"].unique())})\n",
    "borough_df.to_sql(name=\"borough_table\", con=engine, if_exists=\"replace\",index=False)\n",
    "\n",
    "##Precinct Table##\n",
    "precinct_df = pd.read_csv(Path.cwd() / 'geographic_district_data' / f'community_boards.csv')\n",
    "precinct_df = precinct_df[[\"Community Board\", \"CB Precinct(s)\",\"Borough\"]]\n",
    "\n",
    "precinct_df.rename(columns={\"CB Precinct(s)\": \"Precinct\"}, inplace=True)\n",
    "precinct_df[\"Precinct\"] = precinct_df[\"Precinct\"].astype(str)\n",
    "precinct_df[\"Precinct\"] = precinct_df[\"Precinct\"].str.split(\",\")\n",
    "precinct_df = precinct_df.explode(\"Precinct\")\n",
    "precinct_df[\"Precinct\"] = precinct_df[\"Precinct\"].str.split(\";\")\n",
    "precinct_df = precinct_df.explode(\"Precinct\")\n",
    "precinct_df[\"Community Board\"] = precinct_df[\"Borough\"] + \" \" + precinct_df[\"Community Board\"]\n",
    "\n",
    "precinct_df[\"Precinct\"] = precinct_df[\"Precinct\"].str.strip()\n",
    "precinct_df = clean_column_names(precinct_df)\n",
    "precinct_df['precinct'] = pd.to_numeric(precinct_df['precinct'], errors='coerce').astype('Int64')\n",
    "precinct_df.drop(columns=[\"borough\"],inplace=True)\n",
    "precinct_df.to_sql(name=\"precinct_data\", con=engine, if_exists=\"replace\",index=False)\n",
    "\n",
    "#School Table#\n",
    "school_df = pd.read_csv(Path.cwd() / 'school_data' / f'school_district.csv')\n",
    "school_df['borough'] = school_df['dbn'].str[2].map({\n",
    "    'M': 'Manhattan',\n",
    "    'K': 'Brooklyn',\n",
    "    'Q': 'Queens',\n",
    "    'X': 'Bronx',\n",
    "    'R': 'Staten Island'\n",
    "})\n",
    "school_df['Community Board'] = school_df['borough'] + ' Community Board ' + school_df['Community Board'].astype(str)\n",
    "school_df = clean_column_names(school_df)\n",
    "school_df.drop(columns=['borough'], inplace=True)\n",
    "school_df.to_sql(name=\"school_data\", con=engine, if_exists=\"replace\",index=False)\n",
    "\n",
    "engine.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00add1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
